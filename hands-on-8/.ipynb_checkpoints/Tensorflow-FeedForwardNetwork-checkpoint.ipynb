{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to implementing deep feedforward networks in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview:\n",
    "\n",
    "1) Quick introduction to TensorFlow\n",
    "\n",
    "2) Implementing a deep feedforward network in TensorFlow high-level API on the MNIST data set\n",
    "\n",
    "3) Implementing the same network in the TensorFlow low-level API on the same data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 9: Up and Running with TensorFlow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple computational graph\n",
    "\n",
    "![A simple computational graph](figures/simplecompgraph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Your First Graph and Running It in a Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction phase --- Set up a computational graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(4, name=\"y\")\n",
    "f = x*x*y + y + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('.')\n",
    "writer.add_graph(tf.get_default_graph())\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch tensorboard with tensorboard --logdir ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'x:0' shape=() dtype=int32_ref>\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution phase --- Create a session, initialize the variables, and run these through the computational graph: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "result = sess.run(f)\n",
    "print(result)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Express the same execution phase of the program using **with** block to avoid calling Session object repeatedly, explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Establish default session object within the with block:\n",
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()\n",
    "    \n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing all variables at once can be done by adding an initialization node to the computational graph during construction phase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer() # prepare an init node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# Outside Jupyter:\n",
    "#with tf.Session() as sess:\n",
    "#    init.run() # acually initialize all the variables\n",
    "#    result = f.eval()\n",
    "\n",
    "# Inside Jupyter -- obviates the need for the with-block: \n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "init.run()\n",
    "result = f.eval()\n",
    "print(result)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow programs are typically split into\n",
    "\n",
    "1) Construction phase -- which sets up the computational graph of the (ML) algorithm;\n",
    "\n",
    "2) Execution phase -- which executes the computational graph, e.g. looping through training steps consisting of\n",
    "\n",
    "    a) evaluation of model, and \n",
    "\n",
    "    b) update of parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nodes (e.g. variables) created in TensorFlow are automatically added to the default (computational) graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tf.Variable(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manage multiple independent graphs by temporarily making each the default graph only while interacting (modifying) them. This can be done by using **with** block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.graph is graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter or Python shell workflow with repeated commands can create duplicate nodes. \n",
    "\n",
    "Solution 1: restart Jupyter kernel.\n",
    "\n",
    "Solution 2: reset default graph by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lifecycle of a Node Value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a Tensor t, calling t.eval() is equivalent to calling tf.get_default_session().run(t). \n",
    "\n",
    "TensorFlow automatically detects and evaluates the nodes that other nodes depend on first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x * 3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(y.eval()) # 10\n",
    "    print(z.eval()) # 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('.')\n",
    "writer.add_graph(tf.get_default_graph())\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, the evaluations of y and z are separate runs through the computational graph. These two evaluations trace backwards through dependencies and cause the repeated evaluation of both w and x.\n",
    "\n",
    "Within a session: Node values are dropped between graph runs, except variable values are kept between graph runs.\n",
    "\n",
    "To evaluate efficiently and avoid duplicate efforts, evaluate both x and y in one graph run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    y_val, z_val = sess.run([y, z])\n",
    "    print(y_val) # 10\n",
    "    print(z_val) # 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with TensorFlow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow operations (ops) take any number inputs and outputs. Addition and multiplication take two inputs and make one output. Constants and variables take no inputs and are called source ops. Inputs and outputs are multidimensional arrays, i.e. tensors, that have type and shape. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve normal equation for California housing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\hat{\\theta} = ( \\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X} \\mathbf{y}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Math, Latex\n",
    "display(Math(r'\\hat{\\theta} = ( \\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X} \\mathbf{y}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "\n",
    "#reset the default graph to be sure\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# concatenate ones-vector to data to have bias x_0 = 1 for all data points\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data] \n",
    "\n",
    "\n",
    "# Constant node to hold data:\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\") \n",
    "\n",
    "# Constant node to hold corresponding labels/target:\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\") \n",
    "# NB: 1D array housing.target is above reshaped to column vector, and -1 means unspecified, \n",
    "# to be determined from housing.target's length and the other argument(s) of reshape.\n",
    "\n",
    "XT = tf.transpose(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB tf.matmul() is matrix multiplication operation. \n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT,X)), XT), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()\n",
    "    writer = tf.summary.FileWriter('.')\n",
    "    writer.add_graph(tf.get_default_graph())\n",
    "    writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.6894890e+01],\n",
       "       [ 4.3661433e-01],\n",
       "       [ 9.4453208e-03],\n",
       "       [-1.0704148e-01],\n",
       "       [ 6.4345831e-01],\n",
       "       [-3.9632569e-06],\n",
       "       [-3.7880042e-03],\n",
       "       [-4.2093179e-01],\n",
       "       [-4.3400639e-01]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(20640), Dimension(1)])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Perceptron "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Perceptron is an artificial neuron (invented 1957 by Frank Rosenblatt), that takes inputs $x_i$, re-scales them with connection weight $w_{i,j}$ and passes them to an output unit $j$, that linearly combines the products of corresponding inputs and weights, and uses a linear threshold function to output a discrete binary. \n",
    "\n",
    "Multiple output units can compose an output layer to make as many simultaneous binary classifications. \n",
    "\n",
    "Perceptron learning rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle w_{i,j}^\\text{next step} = w_{i,j} + \\alpha (y_j - \\hat{y}_j)x_i$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Math, Latex\n",
    "display(Math(r'w_{i,j}^\\text{next step} = w_{i,j} + \\alpha (y_j - \\hat{y}_j)x_i'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$\\alpha$ is the learning rate, $\\hat{y}_j$ is the output from output unit $j$ with respect to the specific training input vector $\\mathbf{x}$, and $y_j$ is the target output (or label) of the training input vector from the training data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This linear classifier could not express the logical XOR function. (\"Perceptrons\", Minsky & Papert, 1969) Much disappointment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron and Backpropagation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLPs have layers of units (linear threshold units, LTUs) between the layer of input (\"passthrough\") units and the layer output units. \n",
    "\n",
    "Input and hidden layers (but not output layer) is fully connected to the next layer. Likewise, the non-output layers all have a bias neuron. \n",
    "\n",
    "Two or more hidden layers makes an ANN into a DNN. \n",
    "\n",
    "Training MLPs was solved with the backpropagation training algorithm (Rumelhart et al., 1986; Werbos 1974). This algorithm traces backwards, from output to input, the contribution to error (or error gradient) from each connection weight.\n",
    "\n",
    "(Backpropagation == \"Gradient Descent using reverse-mode autodiff.\")\n",
    "\n",
    "Having established error gradient on all weights, a gradient descent step on all weights completes the learning step. \n",
    "\n",
    "This requires differentiable activation functions on the units, so LTUs were replaces with the logistic function. \n",
    "\n",
    "$\\sigma(z) = \\frac{1}{1+exp(-z)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A modern Multi-Layer Perceptron:\n",
    "\n",
    "![A modern Multi-Layer Perceptron](figures/modernMLP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interlude: The MNIST data set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST data set is a set of 70 000 labelled  images of hand-written digits (0 through 9). \n",
    "\n",
    "Commonly used in machine learning/image processing tutorials.\n",
    "\n",
    "The images are small (28 x 28 pixels) and monochrome, with each pixel taking a value between 0 (white) and black (255).\n",
    "\n",
    "Some examples:\n",
    "\n",
    "\n",
    "![Some MNIST examples](figures/mnist.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'target': array(['5', '0', '4', ..., '4', '5', '6'], dtype=object),\n",
       " 'frame': None,\n",
       " 'feature_names': ['pixel1',\n",
       "  'pixel2',\n",
       "  'pixel3',\n",
       "  'pixel4',\n",
       "  'pixel5',\n",
       "  'pixel6',\n",
       "  'pixel7',\n",
       "  'pixel8',\n",
       "  'pixel9',\n",
       "  'pixel10',\n",
       "  'pixel11',\n",
       "  'pixel12',\n",
       "  'pixel13',\n",
       "  'pixel14',\n",
       "  'pixel15',\n",
       "  'pixel16',\n",
       "  'pixel17',\n",
       "  'pixel18',\n",
       "  'pixel19',\n",
       "  'pixel20',\n",
       "  'pixel21',\n",
       "  'pixel22',\n",
       "  'pixel23',\n",
       "  'pixel24',\n",
       "  'pixel25',\n",
       "  'pixel26',\n",
       "  'pixel27',\n",
       "  'pixel28',\n",
       "  'pixel29',\n",
       "  'pixel30',\n",
       "  'pixel31',\n",
       "  'pixel32',\n",
       "  'pixel33',\n",
       "  'pixel34',\n",
       "  'pixel35',\n",
       "  'pixel36',\n",
       "  'pixel37',\n",
       "  'pixel38',\n",
       "  'pixel39',\n",
       "  'pixel40',\n",
       "  'pixel41',\n",
       "  'pixel42',\n",
       "  'pixel43',\n",
       "  'pixel44',\n",
       "  'pixel45',\n",
       "  'pixel46',\n",
       "  'pixel47',\n",
       "  'pixel48',\n",
       "  'pixel49',\n",
       "  'pixel50',\n",
       "  'pixel51',\n",
       "  'pixel52',\n",
       "  'pixel53',\n",
       "  'pixel54',\n",
       "  'pixel55',\n",
       "  'pixel56',\n",
       "  'pixel57',\n",
       "  'pixel58',\n",
       "  'pixel59',\n",
       "  'pixel60',\n",
       "  'pixel61',\n",
       "  'pixel62',\n",
       "  'pixel63',\n",
       "  'pixel64',\n",
       "  'pixel65',\n",
       "  'pixel66',\n",
       "  'pixel67',\n",
       "  'pixel68',\n",
       "  'pixel69',\n",
       "  'pixel70',\n",
       "  'pixel71',\n",
       "  'pixel72',\n",
       "  'pixel73',\n",
       "  'pixel74',\n",
       "  'pixel75',\n",
       "  'pixel76',\n",
       "  'pixel77',\n",
       "  'pixel78',\n",
       "  'pixel79',\n",
       "  'pixel80',\n",
       "  'pixel81',\n",
       "  'pixel82',\n",
       "  'pixel83',\n",
       "  'pixel84',\n",
       "  'pixel85',\n",
       "  'pixel86',\n",
       "  'pixel87',\n",
       "  'pixel88',\n",
       "  'pixel89',\n",
       "  'pixel90',\n",
       "  'pixel91',\n",
       "  'pixel92',\n",
       "  'pixel93',\n",
       "  'pixel94',\n",
       "  'pixel95',\n",
       "  'pixel96',\n",
       "  'pixel97',\n",
       "  'pixel98',\n",
       "  'pixel99',\n",
       "  'pixel100',\n",
       "  'pixel101',\n",
       "  'pixel102',\n",
       "  'pixel103',\n",
       "  'pixel104',\n",
       "  'pixel105',\n",
       "  'pixel106',\n",
       "  'pixel107',\n",
       "  'pixel108',\n",
       "  'pixel109',\n",
       "  'pixel110',\n",
       "  'pixel111',\n",
       "  'pixel112',\n",
       "  'pixel113',\n",
       "  'pixel114',\n",
       "  'pixel115',\n",
       "  'pixel116',\n",
       "  'pixel117',\n",
       "  'pixel118',\n",
       "  'pixel119',\n",
       "  'pixel120',\n",
       "  'pixel121',\n",
       "  'pixel122',\n",
       "  'pixel123',\n",
       "  'pixel124',\n",
       "  'pixel125',\n",
       "  'pixel126',\n",
       "  'pixel127',\n",
       "  'pixel128',\n",
       "  'pixel129',\n",
       "  'pixel130',\n",
       "  'pixel131',\n",
       "  'pixel132',\n",
       "  'pixel133',\n",
       "  'pixel134',\n",
       "  'pixel135',\n",
       "  'pixel136',\n",
       "  'pixel137',\n",
       "  'pixel138',\n",
       "  'pixel139',\n",
       "  'pixel140',\n",
       "  'pixel141',\n",
       "  'pixel142',\n",
       "  'pixel143',\n",
       "  'pixel144',\n",
       "  'pixel145',\n",
       "  'pixel146',\n",
       "  'pixel147',\n",
       "  'pixel148',\n",
       "  'pixel149',\n",
       "  'pixel150',\n",
       "  'pixel151',\n",
       "  'pixel152',\n",
       "  'pixel153',\n",
       "  'pixel154',\n",
       "  'pixel155',\n",
       "  'pixel156',\n",
       "  'pixel157',\n",
       "  'pixel158',\n",
       "  'pixel159',\n",
       "  'pixel160',\n",
       "  'pixel161',\n",
       "  'pixel162',\n",
       "  'pixel163',\n",
       "  'pixel164',\n",
       "  'pixel165',\n",
       "  'pixel166',\n",
       "  'pixel167',\n",
       "  'pixel168',\n",
       "  'pixel169',\n",
       "  'pixel170',\n",
       "  'pixel171',\n",
       "  'pixel172',\n",
       "  'pixel173',\n",
       "  'pixel174',\n",
       "  'pixel175',\n",
       "  'pixel176',\n",
       "  'pixel177',\n",
       "  'pixel178',\n",
       "  'pixel179',\n",
       "  'pixel180',\n",
       "  'pixel181',\n",
       "  'pixel182',\n",
       "  'pixel183',\n",
       "  'pixel184',\n",
       "  'pixel185',\n",
       "  'pixel186',\n",
       "  'pixel187',\n",
       "  'pixel188',\n",
       "  'pixel189',\n",
       "  'pixel190',\n",
       "  'pixel191',\n",
       "  'pixel192',\n",
       "  'pixel193',\n",
       "  'pixel194',\n",
       "  'pixel195',\n",
       "  'pixel196',\n",
       "  'pixel197',\n",
       "  'pixel198',\n",
       "  'pixel199',\n",
       "  'pixel200',\n",
       "  'pixel201',\n",
       "  'pixel202',\n",
       "  'pixel203',\n",
       "  'pixel204',\n",
       "  'pixel205',\n",
       "  'pixel206',\n",
       "  'pixel207',\n",
       "  'pixel208',\n",
       "  'pixel209',\n",
       "  'pixel210',\n",
       "  'pixel211',\n",
       "  'pixel212',\n",
       "  'pixel213',\n",
       "  'pixel214',\n",
       "  'pixel215',\n",
       "  'pixel216',\n",
       "  'pixel217',\n",
       "  'pixel218',\n",
       "  'pixel219',\n",
       "  'pixel220',\n",
       "  'pixel221',\n",
       "  'pixel222',\n",
       "  'pixel223',\n",
       "  'pixel224',\n",
       "  'pixel225',\n",
       "  'pixel226',\n",
       "  'pixel227',\n",
       "  'pixel228',\n",
       "  'pixel229',\n",
       "  'pixel230',\n",
       "  'pixel231',\n",
       "  'pixel232',\n",
       "  'pixel233',\n",
       "  'pixel234',\n",
       "  'pixel235',\n",
       "  'pixel236',\n",
       "  'pixel237',\n",
       "  'pixel238',\n",
       "  'pixel239',\n",
       "  'pixel240',\n",
       "  'pixel241',\n",
       "  'pixel242',\n",
       "  'pixel243',\n",
       "  'pixel244',\n",
       "  'pixel245',\n",
       "  'pixel246',\n",
       "  'pixel247',\n",
       "  'pixel248',\n",
       "  'pixel249',\n",
       "  'pixel250',\n",
       "  'pixel251',\n",
       "  'pixel252',\n",
       "  'pixel253',\n",
       "  'pixel254',\n",
       "  'pixel255',\n",
       "  'pixel256',\n",
       "  'pixel257',\n",
       "  'pixel258',\n",
       "  'pixel259',\n",
       "  'pixel260',\n",
       "  'pixel261',\n",
       "  'pixel262',\n",
       "  'pixel263',\n",
       "  'pixel264',\n",
       "  'pixel265',\n",
       "  'pixel266',\n",
       "  'pixel267',\n",
       "  'pixel268',\n",
       "  'pixel269',\n",
       "  'pixel270',\n",
       "  'pixel271',\n",
       "  'pixel272',\n",
       "  'pixel273',\n",
       "  'pixel274',\n",
       "  'pixel275',\n",
       "  'pixel276',\n",
       "  'pixel277',\n",
       "  'pixel278',\n",
       "  'pixel279',\n",
       "  'pixel280',\n",
       "  'pixel281',\n",
       "  'pixel282',\n",
       "  'pixel283',\n",
       "  'pixel284',\n",
       "  'pixel285',\n",
       "  'pixel286',\n",
       "  'pixel287',\n",
       "  'pixel288',\n",
       "  'pixel289',\n",
       "  'pixel290',\n",
       "  'pixel291',\n",
       "  'pixel292',\n",
       "  'pixel293',\n",
       "  'pixel294',\n",
       "  'pixel295',\n",
       "  'pixel296',\n",
       "  'pixel297',\n",
       "  'pixel298',\n",
       "  'pixel299',\n",
       "  'pixel300',\n",
       "  'pixel301',\n",
       "  'pixel302',\n",
       "  'pixel303',\n",
       "  'pixel304',\n",
       "  'pixel305',\n",
       "  'pixel306',\n",
       "  'pixel307',\n",
       "  'pixel308',\n",
       "  'pixel309',\n",
       "  'pixel310',\n",
       "  'pixel311',\n",
       "  'pixel312',\n",
       "  'pixel313',\n",
       "  'pixel314',\n",
       "  'pixel315',\n",
       "  'pixel316',\n",
       "  'pixel317',\n",
       "  'pixel318',\n",
       "  'pixel319',\n",
       "  'pixel320',\n",
       "  'pixel321',\n",
       "  'pixel322',\n",
       "  'pixel323',\n",
       "  'pixel324',\n",
       "  'pixel325',\n",
       "  'pixel326',\n",
       "  'pixel327',\n",
       "  'pixel328',\n",
       "  'pixel329',\n",
       "  'pixel330',\n",
       "  'pixel331',\n",
       "  'pixel332',\n",
       "  'pixel333',\n",
       "  'pixel334',\n",
       "  'pixel335',\n",
       "  'pixel336',\n",
       "  'pixel337',\n",
       "  'pixel338',\n",
       "  'pixel339',\n",
       "  'pixel340',\n",
       "  'pixel341',\n",
       "  'pixel342',\n",
       "  'pixel343',\n",
       "  'pixel344',\n",
       "  'pixel345',\n",
       "  'pixel346',\n",
       "  'pixel347',\n",
       "  'pixel348',\n",
       "  'pixel349',\n",
       "  'pixel350',\n",
       "  'pixel351',\n",
       "  'pixel352',\n",
       "  'pixel353',\n",
       "  'pixel354',\n",
       "  'pixel355',\n",
       "  'pixel356',\n",
       "  'pixel357',\n",
       "  'pixel358',\n",
       "  'pixel359',\n",
       "  'pixel360',\n",
       "  'pixel361',\n",
       "  'pixel362',\n",
       "  'pixel363',\n",
       "  'pixel364',\n",
       "  'pixel365',\n",
       "  'pixel366',\n",
       "  'pixel367',\n",
       "  'pixel368',\n",
       "  'pixel369',\n",
       "  'pixel370',\n",
       "  'pixel371',\n",
       "  'pixel372',\n",
       "  'pixel373',\n",
       "  'pixel374',\n",
       "  'pixel375',\n",
       "  'pixel376',\n",
       "  'pixel377',\n",
       "  'pixel378',\n",
       "  'pixel379',\n",
       "  'pixel380',\n",
       "  'pixel381',\n",
       "  'pixel382',\n",
       "  'pixel383',\n",
       "  'pixel384',\n",
       "  'pixel385',\n",
       "  'pixel386',\n",
       "  'pixel387',\n",
       "  'pixel388',\n",
       "  'pixel389',\n",
       "  'pixel390',\n",
       "  'pixel391',\n",
       "  'pixel392',\n",
       "  'pixel393',\n",
       "  'pixel394',\n",
       "  'pixel395',\n",
       "  'pixel396',\n",
       "  'pixel397',\n",
       "  'pixel398',\n",
       "  'pixel399',\n",
       "  'pixel400',\n",
       "  'pixel401',\n",
       "  'pixel402',\n",
       "  'pixel403',\n",
       "  'pixel404',\n",
       "  'pixel405',\n",
       "  'pixel406',\n",
       "  'pixel407',\n",
       "  'pixel408',\n",
       "  'pixel409',\n",
       "  'pixel410',\n",
       "  'pixel411',\n",
       "  'pixel412',\n",
       "  'pixel413',\n",
       "  'pixel414',\n",
       "  'pixel415',\n",
       "  'pixel416',\n",
       "  'pixel417',\n",
       "  'pixel418',\n",
       "  'pixel419',\n",
       "  'pixel420',\n",
       "  'pixel421',\n",
       "  'pixel422',\n",
       "  'pixel423',\n",
       "  'pixel424',\n",
       "  'pixel425',\n",
       "  'pixel426',\n",
       "  'pixel427',\n",
       "  'pixel428',\n",
       "  'pixel429',\n",
       "  'pixel430',\n",
       "  'pixel431',\n",
       "  'pixel432',\n",
       "  'pixel433',\n",
       "  'pixel434',\n",
       "  'pixel435',\n",
       "  'pixel436',\n",
       "  'pixel437',\n",
       "  'pixel438',\n",
       "  'pixel439',\n",
       "  'pixel440',\n",
       "  'pixel441',\n",
       "  'pixel442',\n",
       "  'pixel443',\n",
       "  'pixel444',\n",
       "  'pixel445',\n",
       "  'pixel446',\n",
       "  'pixel447',\n",
       "  'pixel448',\n",
       "  'pixel449',\n",
       "  'pixel450',\n",
       "  'pixel451',\n",
       "  'pixel452',\n",
       "  'pixel453',\n",
       "  'pixel454',\n",
       "  'pixel455',\n",
       "  'pixel456',\n",
       "  'pixel457',\n",
       "  'pixel458',\n",
       "  'pixel459',\n",
       "  'pixel460',\n",
       "  'pixel461',\n",
       "  'pixel462',\n",
       "  'pixel463',\n",
       "  'pixel464',\n",
       "  'pixel465',\n",
       "  'pixel466',\n",
       "  'pixel467',\n",
       "  'pixel468',\n",
       "  'pixel469',\n",
       "  'pixel470',\n",
       "  'pixel471',\n",
       "  'pixel472',\n",
       "  'pixel473',\n",
       "  'pixel474',\n",
       "  'pixel475',\n",
       "  'pixel476',\n",
       "  'pixel477',\n",
       "  'pixel478',\n",
       "  'pixel479',\n",
       "  'pixel480',\n",
       "  'pixel481',\n",
       "  'pixel482',\n",
       "  'pixel483',\n",
       "  'pixel484',\n",
       "  'pixel485',\n",
       "  'pixel486',\n",
       "  'pixel487',\n",
       "  'pixel488',\n",
       "  'pixel489',\n",
       "  'pixel490',\n",
       "  'pixel491',\n",
       "  'pixel492',\n",
       "  'pixel493',\n",
       "  'pixel494',\n",
       "  'pixel495',\n",
       "  'pixel496',\n",
       "  'pixel497',\n",
       "  'pixel498',\n",
       "  'pixel499',\n",
       "  'pixel500',\n",
       "  'pixel501',\n",
       "  'pixel502',\n",
       "  'pixel503',\n",
       "  'pixel504',\n",
       "  'pixel505',\n",
       "  'pixel506',\n",
       "  'pixel507',\n",
       "  'pixel508',\n",
       "  'pixel509',\n",
       "  'pixel510',\n",
       "  'pixel511',\n",
       "  'pixel512',\n",
       "  'pixel513',\n",
       "  'pixel514',\n",
       "  'pixel515',\n",
       "  'pixel516',\n",
       "  'pixel517',\n",
       "  'pixel518',\n",
       "  'pixel519',\n",
       "  'pixel520',\n",
       "  'pixel521',\n",
       "  'pixel522',\n",
       "  'pixel523',\n",
       "  'pixel524',\n",
       "  'pixel525',\n",
       "  'pixel526',\n",
       "  'pixel527',\n",
       "  'pixel528',\n",
       "  'pixel529',\n",
       "  'pixel530',\n",
       "  'pixel531',\n",
       "  'pixel532',\n",
       "  'pixel533',\n",
       "  'pixel534',\n",
       "  'pixel535',\n",
       "  'pixel536',\n",
       "  'pixel537',\n",
       "  'pixel538',\n",
       "  'pixel539',\n",
       "  'pixel540',\n",
       "  'pixel541',\n",
       "  'pixel542',\n",
       "  'pixel543',\n",
       "  'pixel544',\n",
       "  'pixel545',\n",
       "  'pixel546',\n",
       "  'pixel547',\n",
       "  'pixel548',\n",
       "  'pixel549',\n",
       "  'pixel550',\n",
       "  'pixel551',\n",
       "  'pixel552',\n",
       "  'pixel553',\n",
       "  'pixel554',\n",
       "  'pixel555',\n",
       "  'pixel556',\n",
       "  'pixel557',\n",
       "  'pixel558',\n",
       "  'pixel559',\n",
       "  'pixel560',\n",
       "  'pixel561',\n",
       "  'pixel562',\n",
       "  'pixel563',\n",
       "  'pixel564',\n",
       "  'pixel565',\n",
       "  'pixel566',\n",
       "  'pixel567',\n",
       "  'pixel568',\n",
       "  'pixel569',\n",
       "  'pixel570',\n",
       "  'pixel571',\n",
       "  'pixel572',\n",
       "  'pixel573',\n",
       "  'pixel574',\n",
       "  'pixel575',\n",
       "  'pixel576',\n",
       "  'pixel577',\n",
       "  'pixel578',\n",
       "  'pixel579',\n",
       "  'pixel580',\n",
       "  'pixel581',\n",
       "  'pixel582',\n",
       "  'pixel583',\n",
       "  'pixel584',\n",
       "  'pixel585',\n",
       "  'pixel586',\n",
       "  'pixel587',\n",
       "  'pixel588',\n",
       "  'pixel589',\n",
       "  'pixel590',\n",
       "  'pixel591',\n",
       "  'pixel592',\n",
       "  'pixel593',\n",
       "  'pixel594',\n",
       "  'pixel595',\n",
       "  'pixel596',\n",
       "  'pixel597',\n",
       "  'pixel598',\n",
       "  'pixel599',\n",
       "  'pixel600',\n",
       "  'pixel601',\n",
       "  'pixel602',\n",
       "  'pixel603',\n",
       "  'pixel604',\n",
       "  'pixel605',\n",
       "  'pixel606',\n",
       "  'pixel607',\n",
       "  'pixel608',\n",
       "  'pixel609',\n",
       "  'pixel610',\n",
       "  'pixel611',\n",
       "  'pixel612',\n",
       "  'pixel613',\n",
       "  'pixel614',\n",
       "  'pixel615',\n",
       "  'pixel616',\n",
       "  'pixel617',\n",
       "  'pixel618',\n",
       "  'pixel619',\n",
       "  'pixel620',\n",
       "  'pixel621',\n",
       "  'pixel622',\n",
       "  'pixel623',\n",
       "  'pixel624',\n",
       "  'pixel625',\n",
       "  'pixel626',\n",
       "  'pixel627',\n",
       "  'pixel628',\n",
       "  'pixel629',\n",
       "  'pixel630',\n",
       "  'pixel631',\n",
       "  'pixel632',\n",
       "  'pixel633',\n",
       "  'pixel634',\n",
       "  'pixel635',\n",
       "  'pixel636',\n",
       "  'pixel637',\n",
       "  'pixel638',\n",
       "  'pixel639',\n",
       "  'pixel640',\n",
       "  'pixel641',\n",
       "  'pixel642',\n",
       "  'pixel643',\n",
       "  'pixel644',\n",
       "  'pixel645',\n",
       "  'pixel646',\n",
       "  'pixel647',\n",
       "  'pixel648',\n",
       "  'pixel649',\n",
       "  'pixel650',\n",
       "  'pixel651',\n",
       "  'pixel652',\n",
       "  'pixel653',\n",
       "  'pixel654',\n",
       "  'pixel655',\n",
       "  'pixel656',\n",
       "  'pixel657',\n",
       "  'pixel658',\n",
       "  'pixel659',\n",
       "  'pixel660',\n",
       "  'pixel661',\n",
       "  'pixel662',\n",
       "  'pixel663',\n",
       "  'pixel664',\n",
       "  'pixel665',\n",
       "  'pixel666',\n",
       "  'pixel667',\n",
       "  'pixel668',\n",
       "  'pixel669',\n",
       "  'pixel670',\n",
       "  'pixel671',\n",
       "  'pixel672',\n",
       "  'pixel673',\n",
       "  'pixel674',\n",
       "  'pixel675',\n",
       "  'pixel676',\n",
       "  'pixel677',\n",
       "  'pixel678',\n",
       "  'pixel679',\n",
       "  'pixel680',\n",
       "  'pixel681',\n",
       "  'pixel682',\n",
       "  'pixel683',\n",
       "  'pixel684',\n",
       "  'pixel685',\n",
       "  'pixel686',\n",
       "  'pixel687',\n",
       "  'pixel688',\n",
       "  'pixel689',\n",
       "  'pixel690',\n",
       "  'pixel691',\n",
       "  'pixel692',\n",
       "  'pixel693',\n",
       "  'pixel694',\n",
       "  'pixel695',\n",
       "  'pixel696',\n",
       "  'pixel697',\n",
       "  'pixel698',\n",
       "  'pixel699',\n",
       "  'pixel700',\n",
       "  'pixel701',\n",
       "  'pixel702',\n",
       "  'pixel703',\n",
       "  'pixel704',\n",
       "  'pixel705',\n",
       "  'pixel706',\n",
       "  'pixel707',\n",
       "  'pixel708',\n",
       "  'pixel709',\n",
       "  'pixel710',\n",
       "  'pixel711',\n",
       "  'pixel712',\n",
       "  'pixel713',\n",
       "  'pixel714',\n",
       "  'pixel715',\n",
       "  'pixel716',\n",
       "  'pixel717',\n",
       "  'pixel718',\n",
       "  'pixel719',\n",
       "  'pixel720',\n",
       "  'pixel721',\n",
       "  'pixel722',\n",
       "  'pixel723',\n",
       "  'pixel724',\n",
       "  'pixel725',\n",
       "  'pixel726',\n",
       "  'pixel727',\n",
       "  'pixel728',\n",
       "  'pixel729',\n",
       "  'pixel730',\n",
       "  'pixel731',\n",
       "  'pixel732',\n",
       "  'pixel733',\n",
       "  'pixel734',\n",
       "  'pixel735',\n",
       "  'pixel736',\n",
       "  'pixel737',\n",
       "  'pixel738',\n",
       "  'pixel739',\n",
       "  'pixel740',\n",
       "  'pixel741',\n",
       "  'pixel742',\n",
       "  'pixel743',\n",
       "  'pixel744',\n",
       "  'pixel745',\n",
       "  'pixel746',\n",
       "  'pixel747',\n",
       "  'pixel748',\n",
       "  'pixel749',\n",
       "  'pixel750',\n",
       "  'pixel751',\n",
       "  'pixel752',\n",
       "  'pixel753',\n",
       "  'pixel754',\n",
       "  'pixel755',\n",
       "  'pixel756',\n",
       "  'pixel757',\n",
       "  'pixel758',\n",
       "  'pixel759',\n",
       "  'pixel760',\n",
       "  'pixel761',\n",
       "  'pixel762',\n",
       "  'pixel763',\n",
       "  'pixel764',\n",
       "  'pixel765',\n",
       "  'pixel766',\n",
       "  'pixel767',\n",
       "  'pixel768',\n",
       "  'pixel769',\n",
       "  'pixel770',\n",
       "  'pixel771',\n",
       "  'pixel772',\n",
       "  'pixel773',\n",
       "  'pixel774',\n",
       "  'pixel775',\n",
       "  'pixel776',\n",
       "  'pixel777',\n",
       "  'pixel778',\n",
       "  'pixel779',\n",
       "  'pixel780',\n",
       "  'pixel781',\n",
       "  'pixel782',\n",
       "  'pixel783',\n",
       "  'pixel784'],\n",
       " 'target_names': ['class'],\n",
       " 'DESCR': \"**Author**: Yann LeCun, Corinna Cortes, Christopher J.C. Burges  \\n**Source**: [MNIST Website](http://yann.lecun.com/exdb/mnist/) - Date unknown  \\n**Please cite**:  \\n\\nThe MNIST database of handwritten digits with 784 features, raw data available at: http://yann.lecun.com/exdb/mnist/. It can be split in a training set of the first 60,000 examples, and a test set of 10,000 examples  \\n\\nIt is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image. It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting. The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.  \\n\\nWith some classification methods (particularly template-based methods, such as SVM and K-nearest neighbors), the error rate improves when the digits are centered by bounding box rather than center of mass. If you do this kind of pre-processing, you should report it in your publications. The MNIST database was constructed from NIST's NIST originally designated SD-3 as their training set and SD-1 as their test set. However, SD-3 is much cleaner and easier to recognize than SD-1. The reason for this can be found on the fact that SD-3 was collected among Census Bureau employees, while SD-1 was collected among high-school students. Drawing sensible conclusions from learning experiments requires that the result be independent of the choice of training set and test among the complete set of samples. Therefore it was necessary to build a new database by mixing NIST's datasets.  \\n\\nThe MNIST training set is composed of 30,000 patterns from SD-3 and 30,000 patterns from SD-1. Our test set was composed of 5,000 patterns from SD-3 and 5,000 patterns from SD-1. The 60,000 pattern training set contained examples from approximately 250 writers. We made sure that the sets of writers of the training set and test set were disjoint. SD-1 contains 58,527 digit images written by 500 different writers. In contrast to SD-3, where blocks of data from each writer appeared in sequence, the data in SD-1 is scrambled. Writer identities for SD-1 is available and we used this information to unscramble the writers. We then split SD-1 in two: characters written by the first 250 writers went into our new training set. The remaining 250 writers were placed in our test set. Thus we had two sets with nearly 30,000 examples each. The new training set was completed with enough examples from SD-3, starting at pattern # 0, to make a full set of 60,000 training patterns. Similarly, the new test set was completed with SD-3 examples starting at pattern # 35,000 to make a full set with 60,000 test patterns. Only a subset of 10,000 test images (5,000 from SD-1 and 5,000 from SD-3) is available on this site. The full 60,000 sample training set is available.\\n\\nDownloaded from openml.org.\",\n",
       " 'details': {'id': '554',\n",
       "  'name': 'mnist_784',\n",
       "  'version': '1',\n",
       "  'format': 'ARFF',\n",
       "  'upload_date': '2014-09-29T03:28:38',\n",
       "  'licence': 'Public',\n",
       "  'url': 'https://www.openml.org/data/v1/download/52667/mnist_784.arff',\n",
       "  'file_id': '52667',\n",
       "  'default_target_attribute': 'class',\n",
       "  'tag': ['AzurePilot',\n",
       "   'OpenML-CC18',\n",
       "   'OpenML100',\n",
       "   'study_1',\n",
       "   'study_123',\n",
       "   'study_41',\n",
       "   'study_99',\n",
       "   'vision'],\n",
       "  'visibility': 'public',\n",
       "  'status': 'active',\n",
       "  'processing_date': '2018-10-03 21:23:30',\n",
       "  'md5_checksum': '0298d579eb1b86163de7723944c7e495'},\n",
       " 'categories': {},\n",
       " 'url': 'https://www.openml.org/d/554'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From Chapter 3 in the \"Hands-On\" book:\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', cache=False)\n",
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "some_digit = X[36000]\n",
    "some_digit_image = some_digit.reshape(28, 28)\n",
    "\n",
    "plt.imshow(some_digit_image, cmap = matplotlib.cm.binary, interpolation=\"nearest\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[36000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training an MLP with TensorFlow's High-Level API "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High-level API called TF.learn. \n",
    "\n",
    "DNNClassifier makes FNN with any number of hidden layers and a softmax output layer for normalized probabilities over a set of exclusive classes.\n",
    "\n",
    "Example with two hidden layers containing 300 and 100 (ReLU) neurons, respectively, and a 10 neuron softmax output layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\sigma : \\mathcal{R}^K \\rightarrow [0,1]^K$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\sigma (\\mathbf{z})_j = \\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}}\\text{  for } j=1,...,K$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Math, Latex\n",
    "display(Math(r'\\sigma : \\mathcal{R}^K \\rightarrow [0,1]^K'))\n",
    "display(Math(r'\\sigma (\\mathbf{z})_j = \\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}}\\text{  for } j=1,...,K')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-32-72949528e1c6>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/vsetty/repos/BRENDA-new/BRENDA/Server/brenda-env/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/vsetty/repos/BRENDA-new/BRENDA/Server/brenda-env/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /Users/vsetty/repos/BRENDA-new/BRENDA/Server/brenda-env/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /Users/vsetty/repos/BRENDA-new/BRENDA/Server/brenda-env/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/vsetty/repos/BRENDA-new/BRENDA/Server/brenda-env/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "X_train = mnist.train.images\n",
    "X_test = mnist.test.images\n",
    "y_train = mnist.train.labels.astype(\"int\")\n",
    "y_test = mnist.test.labels.astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-33-7d2fd823f051>:5: RunConfig.__init__ (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "When switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.\n",
      "WARNING:tensorflow:From <ipython-input-33-7d2fd823f051>:8: infer_real_valued_columns_from_input (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please specify feature columns explicitly.\n",
      "WARNING:tensorflow:From /Users/vsetty/repos/BRENDA-new/BRENDA/Server/brenda-env/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/estimators/estimator.py:143: setup_train_data_feeder (from tensorflow.contrib.learn.python.learn.learn_io.data_feeder) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /Users/vsetty/repos/BRENDA-new/BRENDA/Server/brenda-env/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/learn_io/data_feeder.py:100: extract_pandas_data (from tensorflow.contrib.learn.python.learn.learn_io.pandas_io) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please access pandas data directly.\n",
      "WARNING:tensorflow:From /Users/vsetty/repos/BRENDA-new/BRENDA/Server/brenda-env/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/learn_io/data_feeder.py:159: DataFeeder.__init__ (from tensorflow.contrib.learn.python.learn.learn_io.data_feeder) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /Users/vsetty/repos/BRENDA-new/BRENDA/Server/brenda-env/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/learn_io/data_feeder.py:340: check_array (from tensorflow.contrib.learn.python.learn.learn_io.data_feeder) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please convert numpy dtypes explicitly.\n",
      "WARNING:tensorflow:From /Users/vsetty/repos/BRENDA-new/BRENDA/Server/brenda-env/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/estimators/estimator.py:183: infer_real_valued_columns_from_input_fn (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please specify feature columns explicitly.\n",
      "WARNING:tensorflow:From /Users/vsetty/repos/BRENDA-new/BRENDA/Server/brenda-env/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/estimators/dnn.py:378: multi_class_head (from tensorflow.contrib.learn.python.learn.estimators.head) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.contrib.estimator.*_head.\n",
      "WARNING:tensorflow:From /Users/vsetty/repos/BRENDA-new/BRENDA/Server/brenda-env/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/estimators/estimator.py:1180: BaseEstimator.__init__ (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please replace uses of any Estimator from tf.contrib.learn with an Estimator from tf.estimator.*\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/gz/5x1w02616cb_b9ff_k1k13tr0000gn/T/tmpz6ja7dl8\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x151010358>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': 42, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/var/folders/gz/5x1w02616cb_b9ff_k1k13tr0000gn/T/tmpz6ja7dl8', '_session_creation_timeout_secs': 7200}\n",
      "WARNING:tensorflow:From <ipython-input-33-7d2fd823f051>:11: SKCompat.__init__ (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to the Estimator interface.\n",
      "WARNING:tensorflow:From /Users/vsetty/repos/BRENDA-new/BRENDA/Server/brenda-env/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/learn_io/data_feeder.py:102: extract_pandas_labels (from tensorflow.contrib.learn.python.learn.learn_io.pandas_io) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please access pandas data directly.\n",
      "WARNING:tensorflow:From /Users/vsetty/repos/BRENDA-new/BRENDA/Server/brenda-env/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:From /Users/vsetty/repos/BRENDA-new/BRENDA/Server/brenda-env/lib/python3.7/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /Users/vsetty/repos/BRENDA-new/BRENDA/Server/brenda-env/lib/python3.7/site-packages/tensorflow_core/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/vsetty/repos/BRENDA-new/BRENDA/Server/brenda-env/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/estimators/head.py:678: ModelFnOps.__new__ (from tensorflow.contrib.learn.python.learn.estimators.model_fn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "When switching to tf.estimator.Estimator, use tf.estimator.EstimatorSpec. You can use the `estimator_spec` method to create an equivalent one.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "WARNING:tensorflow:From /Users/vsetty/repos/BRENDA-new/BRENDA/Server/brenda-env/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /var/folders/gz/5x1w02616cb_b9ff_k1k13tr0000gn/T/tmpz6ja7dl8/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.3451104, step = 1\n",
      "INFO:tensorflow:global_step/sec: 425.575\n",
      "INFO:tensorflow:loss = 0.31802303, step = 101 (0.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 316.986\n",
      "INFO:tensorflow:loss = 0.30232453, step = 201 (0.316 sec)\n",
      "INFO:tensorflow:global_step/sec: 450.114\n",
      "INFO:tensorflow:loss = 0.38436478, step = 301 (0.222 sec)\n",
      "INFO:tensorflow:global_step/sec: 447.107\n",
      "INFO:tensorflow:loss = 0.24654518, step = 401 (0.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 412.676\n",
      "INFO:tensorflow:loss = 0.25312525, step = 501 (0.242 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 425.168\n",
      "INFO:tensorflow:loss = 0.056900054, step = 601 (0.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 426.17\n",
      "INFO:tensorflow:loss = 0.13211443, step = 701 (0.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 492.065\n",
      "INFO:tensorflow:loss = 0.20757912, step = 801 (0.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 479.849\n",
      "INFO:tensorflow:loss = 0.11839202, step = 901 (0.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 475.375\n",
      "INFO:tensorflow:loss = 0.20252214, step = 1001 (0.210 sec)\n",
      "INFO:tensorflow:global_step/sec: 443.742\n",
      "INFO:tensorflow:loss = 0.18851696, step = 1101 (0.225 sec)\n",
      "INFO:tensorflow:global_step/sec: 403.79\n",
      "INFO:tensorflow:loss = 0.153252, step = 1201 (0.248 sec)\n",
      "INFO:tensorflow:global_step/sec: 474.631\n",
      "INFO:tensorflow:loss = 0.19756195, step = 1301 (0.211 sec)\n",
      "INFO:tensorflow:global_step/sec: 498.663\n",
      "INFO:tensorflow:loss = 0.05277267, step = 1401 (0.201 sec)\n",
      "INFO:tensorflow:global_step/sec: 461.547\n",
      "INFO:tensorflow:loss = 0.101338364, step = 1501 (0.217 sec)\n",
      "INFO:tensorflow:global_step/sec: 484.14\n",
      "INFO:tensorflow:loss = 0.121346466, step = 1601 (0.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 511.723\n",
      "INFO:tensorflow:loss = 0.042570014, step = 1701 (0.195 sec)\n",
      "INFO:tensorflow:global_step/sec: 492.136\n",
      "INFO:tensorflow:loss = 0.117613316, step = 1801 (0.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 495.59\n",
      "INFO:tensorflow:loss = 0.08180826, step = 1901 (0.202 sec)\n",
      "INFO:tensorflow:global_step/sec: 464.397\n",
      "INFO:tensorflow:loss = 0.09008584, step = 2001 (0.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 487.655\n",
      "INFO:tensorflow:loss = 0.034546405, step = 2101 (0.205 sec)\n",
      "INFO:tensorflow:global_step/sec: 487.993\n",
      "INFO:tensorflow:loss = 0.030064797, step = 2201 (0.205 sec)\n",
      "INFO:tensorflow:global_step/sec: 504.111\n",
      "INFO:tensorflow:loss = 0.055297896, step = 2301 (0.198 sec)\n",
      "INFO:tensorflow:global_step/sec: 509.048\n",
      "INFO:tensorflow:loss = 0.042214584, step = 2401 (0.196 sec)\n",
      "INFO:tensorflow:global_step/sec: 483.713\n",
      "INFO:tensorflow:loss = 0.08601213, step = 2501 (0.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 475.717\n",
      "INFO:tensorflow:loss = 0.0588388, step = 2601 (0.210 sec)\n",
      "INFO:tensorflow:global_step/sec: 489.577\n",
      "INFO:tensorflow:loss = 0.02535018, step = 2701 (0.204 sec)\n",
      "INFO:tensorflow:global_step/sec: 480.709\n",
      "INFO:tensorflow:loss = 0.03780351, step = 2801 (0.208 sec)\n",
      "INFO:tensorflow:global_step/sec: 505.088\n",
      "INFO:tensorflow:loss = 0.08029724, step = 2901 (0.198 sec)\n",
      "INFO:tensorflow:global_step/sec: 499.993\n",
      "INFO:tensorflow:loss = 0.018909317, step = 3001 (0.200 sec)\n",
      "INFO:tensorflow:global_step/sec: 508.986\n",
      "INFO:tensorflow:loss = 0.05088946, step = 3101 (0.196 sec)\n",
      "INFO:tensorflow:global_step/sec: 477.765\n",
      "INFO:tensorflow:loss = 0.015837118, step = 3201 (0.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 495.965\n",
      "INFO:tensorflow:loss = 0.020428741, step = 3301 (0.202 sec)\n",
      "INFO:tensorflow:global_step/sec: 405.474\n",
      "INFO:tensorflow:loss = 0.16672371, step = 3401 (0.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 283.021\n",
      "INFO:tensorflow:loss = 0.10610292, step = 3501 (0.353 sec)\n",
      "INFO:tensorflow:global_step/sec: 441.571\n",
      "INFO:tensorflow:loss = 0.13777259, step = 3601 (0.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 501.291\n",
      "INFO:tensorflow:loss = 0.04099331, step = 3701 (0.199 sec)\n",
      "INFO:tensorflow:global_step/sec: 430.2\n",
      "INFO:tensorflow:loss = 0.011518671, step = 3801 (0.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 468.228\n",
      "INFO:tensorflow:loss = 0.057208885, step = 3901 (0.214 sec)\n",
      "INFO:tensorflow:global_step/sec: 460.493\n",
      "INFO:tensorflow:loss = 0.10869343, step = 4001 (0.217 sec)\n",
      "INFO:tensorflow:global_step/sec: 403.041\n",
      "INFO:tensorflow:loss = 0.034081344, step = 4101 (0.249 sec)\n",
      "INFO:tensorflow:global_step/sec: 286.468\n",
      "INFO:tensorflow:loss = 0.045720458, step = 4201 (0.348 sec)\n",
      "INFO:tensorflow:global_step/sec: 427.851\n",
      "INFO:tensorflow:loss = 0.1142224, step = 4301 (0.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 411.945\n",
      "INFO:tensorflow:loss = 0.14347214, step = 4401 (0.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 438.218\n",
      "INFO:tensorflow:loss = 0.02596602, step = 4501 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 378.889\n",
      "INFO:tensorflow:loss = 0.024090905, step = 4601 (0.264 sec)\n",
      "INFO:tensorflow:global_step/sec: 485.335\n",
      "INFO:tensorflow:loss = 0.009727878, step = 4701 (0.206 sec)\n",
      "INFO:tensorflow:global_step/sec: 500.351\n",
      "INFO:tensorflow:loss = 0.028038833, step = 4801 (0.200 sec)\n",
      "INFO:tensorflow:global_step/sec: 492.988\n",
      "INFO:tensorflow:loss = 0.056126833, step = 4901 (0.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 487.339\n",
      "INFO:tensorflow:loss = 0.073916584, step = 5001 (0.205 sec)\n",
      "INFO:tensorflow:global_step/sec: 488.51\n",
      "INFO:tensorflow:loss = 0.009035141, step = 5101 (0.205 sec)\n",
      "INFO:tensorflow:global_step/sec: 478.339\n",
      "INFO:tensorflow:loss = 0.052173086, step = 5201 (0.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 504.997\n",
      "INFO:tensorflow:loss = 0.031412438, step = 5301 (0.198 sec)\n",
      "INFO:tensorflow:global_step/sec: 409.048\n",
      "INFO:tensorflow:loss = 0.026847428, step = 5401 (0.246 sec)\n",
      "INFO:tensorflow:global_step/sec: 281.163\n",
      "INFO:tensorflow:loss = 0.03403291, step = 5501 (0.355 sec)\n",
      "INFO:tensorflow:global_step/sec: 417.467\n",
      "INFO:tensorflow:loss = 0.046197854, step = 5601 (0.239 sec)\n",
      "INFO:tensorflow:global_step/sec: 490.366\n",
      "INFO:tensorflow:loss = 0.012735528, step = 5701 (0.204 sec)\n",
      "INFO:tensorflow:global_step/sec: 465.702\n",
      "INFO:tensorflow:loss = 0.009732411, step = 5801 (0.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 449.939\n",
      "INFO:tensorflow:loss = 0.0757069, step = 5901 (0.222 sec)\n",
      "INFO:tensorflow:global_step/sec: 491.872\n",
      "INFO:tensorflow:loss = 0.06920184, step = 6001 (0.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 480.797\n",
      "INFO:tensorflow:loss = 0.015911855, step = 6101 (0.208 sec)\n",
      "INFO:tensorflow:global_step/sec: 489.008\n",
      "INFO:tensorflow:loss = 0.01746631, step = 6201 (0.204 sec)\n",
      "INFO:tensorflow:global_step/sec: 502.197\n",
      "INFO:tensorflow:loss = 0.047636107, step = 6301 (0.199 sec)\n",
      "INFO:tensorflow:global_step/sec: 495.626\n",
      "INFO:tensorflow:loss = 0.037202932, step = 6401 (0.202 sec)\n",
      "INFO:tensorflow:global_step/sec: 486.206\n",
      "INFO:tensorflow:loss = 0.0202524, step = 6501 (0.206 sec)\n",
      "INFO:tensorflow:global_step/sec: 505.656\n",
      "INFO:tensorflow:loss = 0.010995091, step = 6601 (0.198 sec)\n",
      "INFO:tensorflow:global_step/sec: 481.649\n",
      "INFO:tensorflow:loss = 0.011111837, step = 6701 (0.208 sec)\n",
      "INFO:tensorflow:global_step/sec: 350.446\n",
      "INFO:tensorflow:loss = 0.012808588, step = 6801 (0.286 sec)\n",
      "INFO:tensorflow:global_step/sec: 341.885\n",
      "INFO:tensorflow:loss = 0.0077955895, step = 6901 (0.292 sec)\n",
      "INFO:tensorflow:global_step/sec: 427.337\n",
      "INFO:tensorflow:loss = 0.023221683, step = 7001 (0.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 414.261\n",
      "INFO:tensorflow:loss = 0.008407045, step = 7101 (0.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 422.547\n",
      "INFO:tensorflow:loss = 0.039222885, step = 7201 (0.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 494.734\n",
      "INFO:tensorflow:loss = 0.004865154, step = 7301 (0.202 sec)\n",
      "INFO:tensorflow:global_step/sec: 478.185\n",
      "INFO:tensorflow:loss = 0.015432077, step = 7401 (0.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 483.25\n",
      "INFO:tensorflow:loss = 0.005481147, step = 7501 (0.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 480.539\n",
      "INFO:tensorflow:loss = 0.041120492, step = 7601 (0.208 sec)\n",
      "INFO:tensorflow:global_step/sec: 458.539\n",
      "INFO:tensorflow:loss = 0.010966438, step = 7701 (0.218 sec)\n",
      "INFO:tensorflow:global_step/sec: 469.407\n",
      "INFO:tensorflow:loss = 0.0049202256, step = 7801 (0.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 485.295\n",
      "INFO:tensorflow:loss = 0.025650047, step = 7901 (0.206 sec)\n",
      "INFO:tensorflow:global_step/sec: 482.626\n",
      "INFO:tensorflow:loss = 0.0069411127, step = 8001 (0.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 469.786\n",
      "INFO:tensorflow:loss = 0.018348577, step = 8101 (0.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 426.903\n",
      "INFO:tensorflow:loss = 0.026050158, step = 8201 (0.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 476.222\n",
      "INFO:tensorflow:loss = 0.030083384, step = 8301 (0.210 sec)\n"
     ]
    }
   ],
   "source": [
    "# Continuing with DNN, from Chapter 10:\n",
    "import tensorflow as tf\n",
    "\n",
    "# From github.com/ageron ch 10 notebook: {\n",
    "config = tf.contrib.learn.RunConfig(tf_random_seed=42)\n",
    "# }\n",
    "\n",
    "feature_cols = tf.contrib.learn.infer_real_valued_columns_from_input(X_train)\n",
    "dnn_clf = tf.contrib.learn.DNNClassifier(hidden_units=[300,100], n_classes=10, \n",
    "                                         feature_columns=feature_cols, config=config)\n",
    "dnn_clf = tf.contrib.learn.SKCompat(dnn_clf) # if TensorFlow >= 1.1\n",
    "dnn_clf.fit(X_train, y_train, batch_size=50, steps=40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = dnn_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred['classes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: clf stands for classifier. Common in sklearn and tensorflow example code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a DNN Using Plain TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower-level Python API, gives more control. \n",
    "\n",
    "Same model as before, with Mini-Batch Gradient Descent to train on the MNIST data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction Phase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "n_inputs = 28*28 # MNIST \n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\") # None: We don't know size of training set/batch. \n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") # Ditto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('.')\n",
    "writer.add_graph(tf.get_default_graph())\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X$ acts as input layer and will be iteratively replaced by training batches. \n",
    "\n",
    "Define function for creating subsequent layers, one at a time: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1]) # Get the number of columns in X, i.e. dimension of each input vector. \n",
    "        stddev = 2 / np.sqrt(n_inputs)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        W = tf.Variable(init, name=\"kernel\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\")\n",
    "        Z = tf.matmul(X, W) + b # broadcasting: b is added to every *row* of matrix tf.matmul(X, W).\n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name_scope is useful for organizing the visualization of the graph using TensorBoard.\n",
    "\n",
    "$W$ is the weights matrix, or \"kernel\", for all the connection weights between inputs (previous layer units?) and neurons in the layer here being defined. \n",
    "\n",
    "Weights are initialized randomly from a truncated normal distribution with standard deviation $\\frac{2}{\\sqrt{n_\\text{inputs}}}$. Randomization helps avoid unbreakable symmetry.\n",
    "\n",
    "Bias initialized to $0$, as it has no symmetry issues. \n",
    "\n",
    "Activation parameter could be provided, such as tf.nn.relu.\n",
    "\n",
    "Creating neuron layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = neuron_layer(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "    hidden2 = neuron_layer(hidden1, n_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "    logits = neuron_layer(hidden2, n_outputs, name=\"outputs\") # before Softmax, to be handled later for optimization reasons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, use pre-defined TensorFlow function tf.layers.dense():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\") # before Softmax, to be handled later for optimization reasons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax output lends itself to training with a cross entropy cost function. If the classifier model assigns low probability to the correct class, the cross entropy cost function penalizes the model. \n",
    "\n",
    "The built-in TensorFlow function sparse_softmax_cross_entropy_with_logits() assumes logits as output from network before passing to softmax activation function. \n",
    "   - Labels are expected in the form $y \\in \\{ 0,1,...,(n_\\text{classes}-1) \\}$. \n",
    "   - The function returns a 1D tensor with cross entropy for each training input vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('.')\n",
    "writer.add_graph(tf.get_default_graph())\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\") # Get mean cross entropy over all training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('.')\n",
    "writer.add_graph(tf.get_default_graph())\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('.')\n",
    "writer.add_graph(tf.get_default_graph())\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) # Cast Boolean True/False values as float to take the mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('.')\n",
    "writer.add_graph(tf.get_default_graph())\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver() # Saving trained model parameters to disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('.')\n",
    "writer.add_graph(tf.get_default_graph())\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution Phases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  Train accuracy:  0.96  Test accuracy:  0.905\n",
      "1  Train accuracy:  0.94  Test accuracy:  0.921\n",
      "2  Train accuracy:  0.96  Test accuracy:  0.929\n",
      "3  Train accuracy:  0.98  Test accuracy:  0.9371\n",
      "4  Train accuracy:  0.96  Test accuracy:  0.9432\n",
      "5  Train accuracy:  0.92  Test accuracy:  0.9475\n",
      "6  Train accuracy:  1.0  Test accuracy:  0.9492\n",
      "7  Train accuracy:  0.92  Test accuracy:  0.9544\n",
      "8  Train accuracy:  0.92  Test accuracy:  0.9561\n",
      "9  Train accuracy:  0.98  Test accuracy:  0.9569\n",
      "10  Train accuracy:  0.98  Test accuracy:  0.9594\n",
      "11  Train accuracy:  1.0  Test accuracy:  0.9624\n",
      "12  Train accuracy:  0.98  Test accuracy:  0.9637\n",
      "13  Train accuracy:  1.0  Test accuracy:  0.9647\n",
      "14  Train accuracy:  0.94  Test accuracy:  0.9678\n",
      "15  Train accuracy:  0.98  Test accuracy:  0.9675\n",
      "16  Train accuracy:  0.96  Test accuracy:  0.9687\n",
      "17  Train accuracy:  0.98  Test accuracy:  0.97\n",
      "18  Train accuracy:  0.92  Test accuracy:  0.9696\n",
      "19  Train accuracy:  1.0  Test accuracy:  0.9712\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, \" Train accuracy: \", acc_train, \" Test accuracy: \", acc_test)\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.x",
   "language": "python",
   "name": "brenda-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
